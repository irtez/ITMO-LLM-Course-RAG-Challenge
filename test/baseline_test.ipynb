{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Для Collab\n",
    "# !pip install langchain==1.1.3 langchain-mistralai==1.1.0 langchain-text-splitters==1.0.0 faiss-cpu==1.13.1 mistralai==1.9.11 langchain-community==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Чтобы скачать трейн датасет документов (если не работает, есть ссылка в README)\n",
    "!wget https://huggingface.co/datasets/irtez/ITMO-LLM-RAG-test/resolve/main/questions_data.zip?download=true -O questions_data.zip\n",
    "!unzip questions_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from typing import List\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Положите ключ в .env файл в директории с ноутбуком или введите его вручную\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = ''\n",
    "assert MISTRAL_API_KEY or os.getenv(\"MISTRAL_API_KEY\"), \"Введите ключ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно использовать любую модель от любого провайдера, Mistral тут для примера\n",
    "chat = ChatMistralAI(\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\") or MISTRAL_API_KEY,\n",
    "    model_name='mistral-large-2407'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2baa5",
   "metadata": {},
   "source": [
    "# Загрузка вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f9b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем вопросы\n",
    "questions = []\n",
    "with open('questions.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        questions.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем метадату документов (в основном, время редактирования - то есть на какой момент документ актуален)\n",
    "# Пока что нигде не используется, но в датасете есть вопросы, связанные со временем\n",
    "with open('docs_metadata.json', 'r') as f:\n",
    "    docs_metadata = json.load(f)\n",
    "docs_metadata['7.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не заработает для валидационного датасета\n",
    "# {q['question_type'] for q in questions}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9681c20",
   "metadata": {},
   "source": [
    "Типы вопросов (`question_type`):\n",
    "- Simple - простой вопрос, например, дата рождения или авторы книги\n",
    "- Simple with condition - простые вопросы с условиями, например, цена акции в определенную дату\n",
    "- Set - ответ на вопрос - это список сущностей (*Какие на земле есть океаны?*)\n",
    "- Multi-hop - вопросы, для ответа на которые нужно сделать несколько \"шагов\" поиска информации, например: *Сколько турниров по всему миру выиграл рекордсмен чемпионата Argentine PGA?* (нужно сначала найти, кто является рекордсменом, а потом - сколько турниров он выиграл, и только затем дать ответ)\n",
    "- False premise - Вопрос поставлен некорректно, верные ответы - \"Я не знаю\", \"Я не могу ответить\", \"Вопрос составлен некорректно\"\n",
    "- Aggregation - для ответа на вопрос нужна аггрегация разных ответов\n",
    "- Comparison - для ответа на вопрос нужно сравнить сущности между собой (*Кто начал выступать раньше, Adele или Ed Sheeran?*)\n",
    "\n",
    "На любые вопросы ответа может не быть (правильный ответ LLM - \"Не знаю\" или \"Не могу ответить из контекста\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06ca87",
   "metadata": {},
   "source": [
    "**ВАЖНО:** типы вопросов, ответы на эти вопросы, а также список документов, релевантных для вопроса (поле `documents`) не будут доступны на валидационном датасете, который будет выдан на паре. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135d5d0",
   "metadata": {},
   "source": [
    "# Загрузка и чанкинг документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_documents(docs_dir: str = \"questions_data\") -> list[Document]:\n",
    "    \"\"\"Загружает все HTML документы из папки.\"\"\"\n",
    "    docs_path = Path(docs_dir)\n",
    "    documents = []\n",
    "    \n",
    "    for file in tqdm(sorted(docs_path.glob(\"*\")), desc=\"Loading documents\"):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        file_name = file.name\n",
    "        # Создаем Document с метаданными\n",
    "        doc_metadata = docs_metadata[file_name]\n",
    "        doc_metadata[\"source\"] = file_name\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata=doc_metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"Загружено документов: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Загружаем все документы\n",
    "all_docs = load_all_documents(docs_dir=\"questions_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем эмбеддинги. Будем использовать all-MiniLM-L6-v2\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda'} # or CPU\n",
    ")\n",
    "\n",
    "# Простой сплиттер\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем документы на чанки\n",
    "all_chunks = text_splitter.split_documents(all_docs)\n",
    "print(f\"Всего чанков: {len(all_chunks)}\")\n",
    "\n",
    "# Подсказка: в датасете много HTML документов. Чтобы уменьшить количество чанков, можно произвести их предобработку (очистку от мусора)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем FAISS индекс\n",
    "vectorstore = FAISS.from_documents(all_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0fda4",
   "metadata": {},
   "source": [
    "# Сам RAG (бейзлайн)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e091de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Промпт для LLM\n",
    "\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a precise question-answering assistant. Your task is to answer questions based ONLY on the provided context documents.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. ONLY use information explicitly stated in the context below\n",
    "2. If the context doesn't contain enough information to answer, respond with \"I cannot answer this question based on the provided information\"\n",
    "3. Do NOT use any prior knowledge - ONLY the context\n",
    "4. Be concise and direct in your answers\n",
    "\n",
    "CONTEXT:\n",
    "{context}\"\"\"\n",
    "\n",
    "RAG_USER_PROMPT = \"\"\"Question: {question}\n",
    "Question time: {question_time}\n",
    "\n",
    "First, identify if the question can be answered from the context above.\n",
    "Then provide your answer.\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", RAG_SYSTEM_PROMPT),\n",
    "    (\"human\", RAG_USER_PROMPT)\n",
    "])\n",
    "\n",
    "# Создаем простую цепочку\n",
    "rag_chain = rag_prompt | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543eef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunks(retrieved_docs: List[Document]) -> None:\n",
    "    for doc in retrieved_docs:\n",
    "        print(f\"Документ '{doc.metadata['source']}'\")\n",
    "        print('Содержимое чанка:\\n')\n",
    "        print(doc.page_content)\n",
    "        print('\\n\\n')\n",
    "\n",
    "# RAG с ретривером\n",
    "def rag_with_retrieval(question_data: dict, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    RAG пайплайн с поиском по FAISS.\n",
    "    \n",
    "    Args:\n",
    "        question_data: словарь с данными вопроса\n",
    "        k: количество чанков для извлечения\n",
    "    \"\"\"\n",
    "    question = question_data['query']\n",
    "    question_time = question_data['query_time']\n",
    "    \n",
    "    # Поиск релевантных чанков\n",
    "    retrieved_docs = vectorstore.similarity_search(question, k=k)\n",
    "    \n",
    "    # Формируем контекст из найденных чанков\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        context_parts.append(f\"[Chunk {i+1} from {source}]\\n{doc.page_content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Вызываем LLM\n",
    "    response = rag_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"question_time\": question_time\n",
    "    })\n",
    "    \n",
    "    return response, retrieved_docs\n",
    "\n",
    "\n",
    "def test_rag(question_data: dict, k: int = 5, verbose: bool = True):\n",
    "    \"\"\"Тестирование RAG с выводом результатов.\"\"\"\n",
    "    response, retrieved_docs = rag_with_retrieval(question_data, k)\n",
    "    \n",
    "    print_chunks(retrieved_docs)\n",
    "    print('\\n------------------------')\n",
    "    question_type = question_data.get('question_type')\n",
    "    if question_type:\n",
    "        print(\"Тип вопроса:\", question_data.get('q'))\n",
    "    print(\"Вопрос:\", question_data['query'])\n",
    "    answer = question_data.get('answer')\n",
    "    if answer:\n",
    "        print(\"Ожидаемый ответ:\", question_data['answer'])\n",
    "    print(\"\\nОтвет RAG-системы:\\n\", response, sep='')\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d659c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_rag(questions[i], k=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663e343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
