# RAG Challenge

RAG-соревнование, в котором нужно найти ответы на вопросы, используя предоставленный корпус документов.

## Разделение на тренировочную и валидационную часть

- Сейчас доступен тренировочный датасет (`questions_data`, `docs_metadata.json`, `questions.jsonl`) и бейзлайн-скрипт (`baseline.ipynb`) в папке `train`
- На паре **20.12** был выложен новый ноутбук `baseline_test.ipynb`, а также новые данные в том же формате, что и `train` (папка `test`)

## Структура репозитория

```
train/
├── baseline.ipynb      # Пример RAG-системы с оценкой качества
├── questions.jsonl     # Вопросы (по одному JSON-объекту на строку)
├── docs_metadata.json  # Метаданные документов (время редактирования)
└── questions_data/     # Папка с документами (0.html, 1.html, ...), нужно скачивать отдельно
test/
├── baseline_test.ipynb # Тот же пример, что и в train
├── questions.jsonl     # Вопросы (по одному JSON-объекту на строку)
├── docs_metadata.json  # Метаданные документов (время редактирования)
└── questions_data/     # Папка с документами (0.html, 1.html, ...), нужно скачивать отдельно
```

> `train/baseline.ipynb` и `test/baseline_test.ipynb` практически идентичны. Сменилась только ссылка на скачивание, несколько ячеек подредактировано из-за отсутствия некоторых полей в JSON, а также была убрана часть с LLM-as-a-Judge.

### Скачивание данных

[Скачать архив с TRAIN документами (questions_data.zip)](https://huggingface.co/datasets/Fourzeroo/ITMO-LLM-Course-RAG/resolve/main/questions_data.zip?download=true)

[Скачать архив с TEST документами (questions_data.zip)](https://huggingface.co/datasets/irtez/ITMO-LLM-RAG-test/resolve/main/questions_data.zip?download=true)

Также можно скачать архивы напрямую из соответствующих ноутбуков (см. `train/baseline.ipynb` и `test/baseline_test.ipynb`).

## Правила сдачи

Ссылка на гугл форму для сдачи ответов будет доступна в чате в Telegram в начале пары.

- Нужно выбрать верный вариант ответа на вопрос (или максимально похожий на тот, который получится у вас). Если подходят несколько вариантов, надо выбрать самый точный на основании доступной информации.
- Отвечать на вопросы нужно именно по представленным документам
- Если ответ на вопрос существует (например, его можно найти в интернете), но в документах его нет - правильный ответ "Нет информации" ("No information")
- Если в вопросе есть "False Premise", то есть вопрос изначально поставлен некорректно, на него тоже нужно отвечать "No information"
- Если вопрос с множественным выбором ответов, то надо либо выбирать либо "No information", либо другие варианты, при выборе "No information" одновременно с другими вариантами ответ будет считаться неправильным.
- Если информация из документов, по вашему мнению, является неправильной (противоречит, например, информации из интернета) - отвечать нужно именно информацией из документов.

## Формат данных

### questions.jsonl

Каждая строка — JSON-объект с полями:

| Поле | Описание |
|------|----------|
| `query` | Текст вопроса |
| `query_time` | Время вопроса (может быть важно для вопросов про актуальные данные) |
| `domain` | Тематика вопроса (sports, music, ...) |
| `question_type` | Тип вопроса (см. ниже) *(только в train)* |
| `answer` | Правильный ответ *(только в train)* |
| `alt_ans` | Альтернативные правильные ответы *(только в train)* |
| `documents` | Список релевантных документов *(только в train)* |

### docs_metadata.json

Словарь вида `{"filename": {"page_last_modified": "..."}}` — содержит время последнего редактирования страницы. Может пригодиться для вопросов, связанных со временем.

### questions_data/

Папка с документами. Большинство документов содержит HTML-разметку — рекомендуется предварительная очистка.

Не все документы могут быть в формате HTML. Еще один возможный формат - `.txt` (HTML-разметки там не будет, только текст).

## Типы вопросов *(только в `train`)*

| Тип | Описание |
|-----|----------|
| `simple` | Простой вопрос с однозначным ответом |
| `simple_w_condition` | Простой вопрос с условиями (например, цена в определенную дату) |
| `set` | Ответ — список сущностей |
| `multi-hop` | Требуется несколько шагов поиска информации |
| `false_premise` | Вопрос некорректен, правильный ответ — "Не знаю" / "Вопрос составлен некорректно" |
| `aggregation` | Требуется агрегация данных из разных источников |
| `comparison` | Требуется сравнение сущностей |

> ⚠️ **Важно:** На любой вопрос ответа может не быть в документах. Правильный ответ в таком случае — "Не знаю" или "Не могу ответить".

## Baseline-решение (baseline.ipynb)

Ноутбук содержит пример простой RAG-системы:

1. **Загрузка данных** — чтение вопросов из `questions.jsonl` и документов из `questions_data/`
2. **Чанкинг** — разбиение документов на чанки с помощью `RecursiveCharacterTextSplitter`
3. **Индексирование** — создание FAISS-индекса с эмбеддингами (`all-MiniLM-L6-v2`)
4. **Retrieval** — поиск релевантных чанков по запросу
5. **Generation** — генерация ответа с помощью LLM (Mistral) на основе найденного контекста
6. **Оценка** *(только в `train`)* — LLM-as-a-Judge для оценки качества ответов (только для train-датасета)

> ⚠️ **Важно:** Часть с оценкой (LLM-as-a-Judge) нужна только для тренировочного датасета, на паре она просто не заработает - правильные ответы на вопросы не будут доступны).

*Примечание по LLM-as-a-Judge: в коде используется Mistral API. Если вы будете использовать другого провайдера, например OpenRouter, можно получать ответы на все вопросы из датасета (и на следующей итерации их оценку судьей) одновременно, что сэкономит время на проверку (Mistral API ограничен 1 запросом в секунду и 500 000 токенами в минуту). Для этого нужно немного переработать функцию `evaluate_all_questions`, раскомментировав строки*

### Используемые библиотеки

```
langchain, langchain-mistralai, langchain-community
faiss-cpu
sentence-transformers
mistralai
```

Для получения эмбеддингов также будут нужны:
```
torch
sentence-transformers
datasets
```

Общие пакеты:
```
tqdm
ipykernel
ipywidgets
```

## Формат оценки

Ответы оцениваются метрикой **Truthfulness**:

| Score | Описание |
|-------|----------|
| 1.0 | Полностью верный ответ |
| 0.5 | Частично верный ответ |
| 0.0 | Отказ от ответа (когда ответ есть) |
| -1.0 | Галлюцинация или уверенный неверный ответ |

## Валидационный датасет (20.12)

Предоставлены новые файлы (папка `test`):
- `questions.jsonl` — новые вопросы (без полей `answer`, `alt_ans`, `documents`, `question_type`)
- `docs_metadata.json` — метаданные новых документов
- `questions_data.zip` — архив с новыми документами
- `baseline_test.ipynb` - новый бейзлайн для тестовых данных

Формат данных на тестовом датасете такой же, поэтому все скрипты, подготовленные на тренировочных данных, должны работать.

## Формат сдачи

- Ответы вводятся в гугл-форму на паре 20.12
- Код решения прикреплять не обязательно
- Выбор инструментов не ограничен
- Объединяться в команды нельзя, ответы могут проверяться на плагиат
