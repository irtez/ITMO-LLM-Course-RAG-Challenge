{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fbd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Для Collab\n",
    "# !pip install langchain==1.1.3 langchain-mistralai==1.1.0 langchain-text-splitters==1.0.0 faiss-cpu==1.13.1 mistralai==1.9.11 langchain-community==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Чтобы скачать трейн датасет документов (если не работает, есть ссылка в README)\n",
    "!wget https://huggingface.co/datasets/Fourzeroo/ITMO-LLM-Course-RAG/resolve/main/questions_data.zip?download=true -O questions_data.zip\n",
    "!unzip questions_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from typing import List\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Положите ключ в .env файл в директории с ноутбуком или введите его вручную\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = ''\n",
    "assert MISTRAL_API_KEY or os.getenv(\"MISTRAL_API_KEY\"), \"Введите ключ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно использовать любую модель от любого провайдера, Mistral тут для примера\n",
    "chat = ChatMistralAI(\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\") or MISTRAL_API_KEY,\n",
    "    model_name='mistral-large-2407'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2baa5",
   "metadata": {},
   "source": [
    "# Загрузка вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f9b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем вопросы\n",
    "questions = []\n",
    "with open('questions.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        questions.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем метадату документов (в основном, время редактирования - то есть на какой момент документ актуален)\n",
    "# Пока что нигде не используется, но в датасете есть вопросы, связанные со временем\n",
    "with open('docs_metadata.json', 'r') as f:\n",
    "    docs_metadata = json.load(f)\n",
    "docs_metadata['7.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример такого вопроса, когда время вопроса/документа может оказаться важным\n",
    "questions[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не заработает для валидационного датасета\n",
    "{q['question_type'] for q in questions}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9681c20",
   "metadata": {},
   "source": [
    "Типы вопросов (`question_type`):\n",
    "- Simple - простой вопрос, например, дата рождения или авторы книги\n",
    "- Simple with condition - простые вопросы с условиями, например, цена акции в определенную дату\n",
    "- Set - ответ на вопрос - это список сущностей (*Какие на земле есть океаны?*)\n",
    "- Multi-hop - вопросы, для ответа на которые нужно сделать несколько \"шагов\" поиска информации, например: *Сколько турниров по всему миру выиграл рекордсмен чемпионата Argentine PGA?* (нужно сначала найти, кто является рекордсменом, а потом - сколько турниров он выиграл, и только затем дать ответ)\n",
    "- False premise - Вопрос поставлен некорректно, верные ответы - \"Я не знаю\", \"Я не могу ответить\", \"Вопрос составлен некорректно\"\n",
    "- Aggregation - для ответа на вопрос нужна аггрегация разных ответов\n",
    "- Comparison - для ответа на вопрос нужно сравнить сущности между собой (*Кто начал выступать раньше, Adele или Ed Sheeran?*)\n",
    "\n",
    "На любые вопросы ответа может не быть (правильный ответ LLM - \"Не знаю\" или \"Не могу ответить из контекста\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06ca87",
   "metadata": {},
   "source": [
    "**ВАЖНО:** типы вопросов, ответы на эти вопросы, а также список документов, релевантных для вопроса (поле `documents`) не будут доступны на валидационном датасете, который будет выдан на паре. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135d5d0",
   "metadata": {},
   "source": [
    "# Загрузка и чанкинг документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_documents(docs_dir: str = \"questions_data\") -> list[Document]:\n",
    "    \"\"\"Загружает все HTML документы из папки.\"\"\"\n",
    "    docs_path = Path(docs_dir)\n",
    "    documents = []\n",
    "    \n",
    "    for file in tqdm(sorted(docs_path.glob(\"*\")), desc=\"Loading documents\"):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        file_name = file.name\n",
    "        # Создаем Document с метаданными\n",
    "        doc_metadata = docs_metadata[file_name]\n",
    "        doc_metadata[\"source\"] = file_name\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata=doc_metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"Загружено документов: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Загружаем все документы\n",
    "all_docs = load_all_documents(docs_dir=\"questions_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем эмбеддинги. Будем использовать all-MiniLM-L6-v2\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda'} # or CPU\n",
    ")\n",
    "\n",
    "# Простой сплиттер\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиваем документы на чанки\n",
    "all_chunks = text_splitter.split_documents(all_docs)\n",
    "print(f\"Всего чанков: {len(all_chunks)}\")\n",
    "\n",
    "# Подсказка: в датасете много HTML документов. Чтобы уменьшить количество чанков, можно произвести их предобработку (очистку от мусора)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем FAISS индекс\n",
    "vectorstore = FAISS.from_documents(all_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0fda4",
   "metadata": {},
   "source": [
    "# Сам RAG (бейзлайн)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e091de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Промпт для LLM\n",
    "\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a precise question-answering assistant. Your task is to answer questions based ONLY on the provided context documents.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. ONLY use information explicitly stated in the context below\n",
    "2. If the context doesn't contain enough information to answer, respond with \"I cannot answer this question based on the provided information\"\n",
    "3. Do NOT use any prior knowledge - ONLY the context\n",
    "4. Be concise and direct in your answers\n",
    "\n",
    "CONTEXT:\n",
    "{context}\"\"\"\n",
    "\n",
    "RAG_USER_PROMPT = \"\"\"Question: {question}\n",
    "Question time: {question_time}\n",
    "\n",
    "First, identify if the question can be answered from the context above.\n",
    "Then provide your answer.\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", RAG_SYSTEM_PROMPT),\n",
    "    (\"human\", RAG_USER_PROMPT)\n",
    "])\n",
    "\n",
    "# Создаем простую цепочку\n",
    "rag_chain = rag_prompt | chat | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543eef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunks(retrieved_docs: List[Document]) -> None:\n",
    "    for doc in retrieved_docs:\n",
    "        print(f\"Документ '{doc.metadata['source']}'\")\n",
    "        print('Содержимое чанка:\\n')\n",
    "        print(doc.page_content)\n",
    "        print('\\n\\n')\n",
    "\n",
    "# RAG с ретривером\n",
    "def rag_with_retrieval(question_data: dict, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    RAG пайплайн с поиском по FAISS.\n",
    "    \n",
    "    Args:\n",
    "        question_data: словарь с данными вопроса\n",
    "        k: количество чанков для извлечения\n",
    "    \"\"\"\n",
    "    question = question_data['query']\n",
    "    question_time = question_data['query_time']\n",
    "    \n",
    "    # Поиск релевантных чанков\n",
    "    retrieved_docs = vectorstore.similarity_search(question, k=k)\n",
    "    \n",
    "    # Формируем контекст из найденных чанков\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        context_parts.append(f\"[Chunk {i+1} from {source}]\\n{doc.page_content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Вызываем LLM\n",
    "    response = rag_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"question_time\": question_time\n",
    "    })\n",
    "    \n",
    "    return response, retrieved_docs\n",
    "\n",
    "\n",
    "def test_rag(question_data: dict, k: int = 5, verbose: bool = True):\n",
    "    \"\"\"Тестирование RAG с выводом результатов.\"\"\"\n",
    "    response, retrieved_docs = rag_with_retrieval(question_data, k)\n",
    "    \n",
    "    print_chunks(retrieved_docs)\n",
    "    print('\\n------------------------')\n",
    "    question_type = question_data.get('question_type')\n",
    "    if question_type:\n",
    "        print(\"Тип вопроса:\", question_data.get('q'))\n",
    "    print(\"Вопрос:\", question_data['query'])\n",
    "    answer = question_data.get('answer')\n",
    "    if answer:\n",
    "        print(\"Ожидаемый ответ:\", question_data['answer'])\n",
    "    print(\"\\nОтвет RAG-системы:\\n\", response, sep='')\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d659c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируем на случайном вопросе\n",
    "i = 2\n",
    "questions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_rag(questions[i], k=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fff19",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge (только для тренировочного датасета)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import asyncio\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3232707",
   "metadata": {},
   "source": [
    "Для оценки своей RAG системы можно воспользоваться вот этим шаблоном, где LLM проверяет ответы на вопросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Структура для structured output\n",
    "class JudgeResponse(BaseModel):\n",
    "    score: Literal[\"1.0\", \"0.5\", \"0.0\", \"-1.0\"] = Field(\n",
    "        description=\"Truthfulness score: 1.0=correct, 0.5=partial, 0.0=refusal, -1.0=hallucination\"\n",
    "    )\n",
    "    reason: str = Field(description=\"Brief explanation of the score (1-2 sentences)\")\n",
    "\n",
    "# Judge prompt для оценки truthfulness\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"You are an impartial judge evaluating the truthfulness of an AI assistant's answer.\n",
    "\n",
    "You will be given:\n",
    "1. A question\n",
    "2. The correct answer(s) - there may be alternative correct answers\n",
    "3. The AI assistant's response\n",
    "\n",
    "Score the response using this scale:\n",
    "- 1.0: Completely correct answer that matches the expected answer\n",
    "- 0.5: Partially correct answer (some correct information but incomplete or with minor errors)\n",
    "- 0.0: Refusal to answer (e.g., \"I don't know\", \"I cannot answer\", \"no information in context\")\n",
    "- -1.0: Hallucination - the AI gave a CONFIDENT FACTUAL ANSWER that is WRONG\n",
    "\n",
    "CRITICAL DISTINCTION between 0.0 and -1.0:\n",
    "- Score 0.0 (Refusal): AI says \"I cannot answer\", \"I don't have information\", \"the context doesn't contain this\" - this is NOT a hallucination, just a refusal\n",
    "- Score -1.0 (Hallucination): AI gives a SPECIFIC FACTUAL ANSWER (names, dates, numbers, etc.) that is INCORRECT\n",
    "\n",
    "SPECIAL CASE - \"invalid question\" / \"false premise\":\n",
    "When the correct answer is \"invalid question\" or similar, the question itself is flawed.\n",
    "- Score 1.0: AI identifies the false premise OR refuses to answer\n",
    "- Score -1.0: AI gives a confident factual answer (hallucination)\n",
    "\n",
    "IMPORTANT:\n",
    "- A refusal is ALWAYS score 0.0 (or 1.0 in SPECIAL CASE above), never -1.0, regardless of whether correct answer exists\n",
    "- Only give -1.0 if the AI states incorrect facts confidently\n",
    "- Consider alternative answers as equally valid\n",
    "- Focus on factual correctness, not style\"\"\"\n",
    "\n",
    "JUDGE_USER_PROMPT = \"\"\"Question: {question}\n",
    "\n",
    "Correct Answer: {correct_answer}\n",
    "Alternative Answers: {alt_answers}\n",
    "\n",
    "AI Response: {response}\n",
    "\n",
    "Evaluate the response:\n",
    "- If AI refused to answer → Score 0.0\n",
    "- If AI gave wrong facts confidently → Score -1.0\n",
    "- If AI answered correctly → Score 1.0 or 0.5\"\"\"\n",
    "\n",
    "judge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", JUDGE_SYSTEM_PROMPT),\n",
    "    (\"human\", JUDGE_USER_PROMPT)\n",
    "])\n",
    "\n",
    "# Используем structured output\n",
    "judge_chain = judge_prompt | chat.with_structured_output(JudgeResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_rag_response_async(question_data: dict, k: int = 10):\n",
    "    question = question_data['query']\n",
    "    question_time = question_data['query_time']\n",
    "    \n",
    "    # Поиск релевантных чанков (синхронный)\n",
    "    retrieved_docs = vectorstore.similarity_search(question, k=k)\n",
    "    \n",
    "    # Формируем контекст\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        context_parts.append(f\"[Chunk {i+1}]\\n{doc.page_content}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Асинхронный вызов LLM с rate limiting\n",
    "    response = await rag_chain.ainvoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"question_time\": question_time\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "async def judge_response_async(question_data: dict, response: str):\n",
    "    \"\"\"Асинхронная оценка ответа с structured output.\"\"\"\n",
    "    alt_answers = question_data.get('alt_ans', [])\n",
    "    alt_answers_str = \", \".join(str(a) for a in alt_answers) if alt_answers else \"None\"\n",
    "    \n",
    "    try:\n",
    "        result: JudgeResponse = await judge_chain.ainvoke({\n",
    "            \"question\": question_data['query'],\n",
    "            \"correct_answer\": question_data['answer'],\n",
    "            \"alt_answers\": alt_answers_str,\n",
    "            \"response\": response\n",
    "        })\n",
    "        return float(result.score), result.reason\n",
    "    except Exception as e:\n",
    "        return 0.0, f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "async def evaluate_all_questions(questions_list: list, k: int = 10):\n",
    "    \n",
    "    # Шаг 1: Получаем все ответы RAG\n",
    "    responses = []\n",
    "    for q in tqdm(questions_list, desc='Questions'):\n",
    "        response = await get_rag_response_async(q, k)\n",
    "        responses.append(response)\n",
    "    # Если нет ограничения на 1 RPS как у Mistral, можно получать ответы на запросы пареллельно\n",
    "\n",
    "    # rag_tasks = [get_rag_response_async(q, k) for q in questions_list]\n",
    "    # responses = await asyncio.gather(*rag_tasks)\n",
    "\n",
    "    # Избегаем ограничения на количество токенов в минуту от Mistral API\n",
    "    print(\"Ждем 60 секунд для сброса лимита по токенам Mistral API...\")\n",
    "    await asyncio.sleep(60)\n",
    "\n",
    "    # Шаг 2: Оцениваем все ответы\n",
    "    evaluations = []\n",
    "    for q, r in tqdm(zip(questions_list, responses), desc='Evaluations', total=len(responses)):\n",
    "        eval = await judge_response_async(q, r)\n",
    "        evaluations.append(eval)\n",
    "    # judge_tasks = [judge_response_async(q, r) for q, r in zip(questions_list, responses)]\n",
    "    # evaluations = await asyncio.gather(*judge_tasks)\n",
    "    \n",
    "    # Собираем результаты\n",
    "    results = []\n",
    "    for q, response, (score, reason) in zip(questions_list, responses, evaluations):\n",
    "        results.append({\n",
    "            'question': q['query'],\n",
    "            'question_type': q['question_type'],\n",
    "            'correct_answer': q['answer'],\n",
    "            'rag_response': response,\n",
    "            'score': score,\n",
    "            'reason': reason\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_summary(results: list):\n",
    "    scores = [r['score'] for r in results]\n",
    "    score_counts = Counter(scores)\n",
    "    \n",
    "    total = len(results)\n",
    "    avg_score = sum(scores) / total if total > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"РЕЗУЛЬТАТЫ ОЦЕНКИ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nВсего вопросов: {total}\")\n",
    "    print(f\"Средний score: {avg_score:.3f}\")\n",
    "    \n",
    "    print(\"\\nРаспределение оценок:\")\n",
    "    for score in [1.0, 0.5, 0.0, -1.0]:\n",
    "        count = score_counts.get(score, 0)\n",
    "        pct = count / total * 100 if total > 0 else 0\n",
    "        bar = \"█\" * int(pct / 2)\n",
    "        print(f\"  {score:>4}: {count:>3} ({pct:>5.1f}%) {bar}\")\n",
    "    \n",
    "    # Разбивка по типам вопросов\n",
    "    print(\"\\nПо типам вопросов:\")\n",
    "    type_scores = {}\n",
    "    for r in results:\n",
    "        qtype = r['question_type']\n",
    "        if qtype not in type_scores:\n",
    "            type_scores[qtype] = []\n",
    "        type_scores[qtype].append(r['score'])\n",
    "    \n",
    "    for qtype, scores_list in sorted(type_scores.items()):\n",
    "        avg = sum(scores_list) / len(scores_list)\n",
    "        print(f\"  {qtype}: avg={avg:.3f} (n={len(scores_list)})\")\n",
    "    \n",
    "    # Примеры ошибок\n",
    "    errors = [r for r in results if r['score'] < 0]\n",
    "    if errors:\n",
    "        print(f\"\\nПримеры галлюцинаций (score=-1):\")\n",
    "        for r in errors[:3]:\n",
    "            print(f\"  Q: {r['question']}\")\n",
    "            print(f\"  Expected: {r['correct_answer']}\")\n",
    "            print(f\"  Got: {r['rag_response']}\")\n",
    "            print(f\"  Reason: {r['reason']}\")\n",
    "            print()\n",
    "    \n",
    "    return avg_score, score_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск оценки на всех вопросах\n",
    "results = await evaluate_all_questions(questions, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score, score_counts = print_evaluation_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663e343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
